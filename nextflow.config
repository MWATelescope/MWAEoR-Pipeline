manifest {
    name = 'nf-core/MWAEoR-Pipeline'
    description = 'MWA EoR pipeline'
}
params {
    obsids_path = 'obsids.csv'
    obsids_suffix = ''
    quality_updates_path = 'quality-updates.csv'
    tile_updates_path = 'tile-updates.csv'
    visName = null

    // //////// //
    // metadata //
    // //////// //

    // set to 1 to include ppds in downloaded metafits
    metafits_incl_ppds = 0
    noppds = true

    sky_chans = 131..154

    filter_sweet_pointings = null
    filter_ew_pointings = null
    filter_min_good_tiles = null
    // (12.5%)
    filter_bad_tile_frac = 16.01 / 128
    filter_dead_dipole_frac = null
    filter_quality = 1
    filter_config = null
    filter_eorfield = null
    filter_eorband = null
    filter_ra = null
    filter_dec = null
    filter_ionoqa = null
    filter_sun_elevation = null
    filter_min_sun_pointing_distance = null

    asvo_api_key = "${MWA_ASVO_API_KEY}"
    // disable all qa
    noqa = false
    nodemo = true

    // ///////////// //
    // preprocessing //
    // ///////////// //

    noprep = false
    prepByCh = false
    // pull prep from acacia if available
    pullPrep = true
    // tag to add to the end of prep files
    prep_suffix = ''
    prep_time_res_s = 2
    prep_freq_res_khz = 40
    prep_rfi = true
    prep_pc = null
    prep_edge_width = null
    van_vleck = false
    // use --nopreqa to disable prepVisQA and flagQA
    noprepqa = false
    noplotprepqa = false
    nossins = false
    // when you want to do prepVisQA but not use its filter
    noprepqafilter = false
    noautoplot = true

    // -> prep vis convert for export
    prep_export_time_res_s = null
    prep_export_freq_res_khz = null
    prep_export_ext = 'uvfits'

    // //////// //
    // flagging //
    // //////// //

    noflag = false
    // add extra ssins plots for each rx
    ssins_by_rx = false
    flag_occupancy_threshold = 0.99
    rfi_occupancy_threshold = 0.25
    ssins_occupancy_threshold = 0.99
    ssins_narrow_threshold = null
    // arbitrary (1308 pass, 2024 fail of 17488)
    ssins_dab_threshold = 0.05
    // median=9% + std=22% * 1 = 31% (7059 pass, 2076 fail, of 17488 total)
    ssins_streak_threshold = 0.31
    ssins_apply = false
    noplotprepqa = false

    // /////////// //
    // calibration //
    // /////////// //

    // use --nocal to skip calibration
    nocal = false
    // use --nophasefits to disable phase fit analysis
    nophasefits = false
    // use --fitphase to apply fitted phases
    fitphase = false
    // pull calsol from acacia if available
    pullCalSol = true
    // use --noManualFlags to disable reading flags from tile updates csv
    noManualFlags = false
    // use --noPrepFlags to disable flags from prepVisQA
    noPrepFlags = false
    // use --noCalFlags to disable flags from calQA
    noCalFlags = false
    nosrclist = true
    noplotcalqa = false
    // string to add to the end of the cal directory
    cal_suffix = ''
    // hyperdrive di-cal args
    // - key: short name that appears in calibration solution filename
    // - value: args for hyperdrive di-cal
    dical_name = "30l_src8k_300it"
    dical_args = "--uvw-min 30l -n 8000 --max-iterations 300 --stop-thresh 1e-20"
    // mwa-reduce calibrate args
    // rexcal_name = "30l_src100"
    // rexcal_args = ""

    // path/of/mwa_full_embedded_element_pattern.h5
    beam_path = '<!!!>'
    // path/of/sourcelist.txt
    sourcelist = '<!!!>'
    hyp_srclist_args = '--source-dist-cutoff=180 --veto-threshold 0.005'
    // hyp_dical_args = "--max-iterations 300 --stop-thresh 1e-8"
    hyp_dical_args = ""
    hyp_sols_plot_args = '--no-ref-tile --max-amp 2'

    hyp_peel_args = '--num-passes 5 --num-loops 6 --convergence 0.5'

    nocalqa = false
    filter_max_rms_convg = null
    filter_max_unused_bl_frac = null
    filter_max_unconvg_chs = null

    // ///// //
    // apply //
    // ///// //
    noapply = false
    apply_time_res = 8
    apply_freq_res = 80
    apply_args = ''
    nosub = true
    noionosub = true
    sub_nsrcs = 8000
    ionosub_nsrcs = 1000
    nocluster = true
    // use --ignore-dut1 in hyperdrive apply
    nodut1 = false

    // file types
    noms = true
    nouv = false

    // ////////// //
    // visibility //
    // ////////// //
    noeor = true
    nopsmetrics = false
    nodelayspec = false
    nopowerspec = false
    noplotvisqa = false
    nouvplot = true

    // args to autoplot script, e.g. "--log_scale --transparent"
    autoplot_args = ''
    // filter (any) visibilities with too much window power
    filter_max_ps_window_unsub = null
    // filter unsubtracted visibilities with too little window power
    filter_min_ps_window_unsub = null
    // filter unsubtracted visibilities with too much window:wedge ratio
    filter_max_ps_ratio_unsub = null
    // filter unsubtracted visibilities with too little window:wedge ratio
    filter_min_ps_ratio_unsub = null
    // filter subtracted visibilities which subtract too little wedge power
    filter_max_ps_wedge_ratio_sub = null
    // filter subtracted visibilities which subtract too much wedge power
    filter_min_ps_wedge_ratio_sub = null
    // filter subtracted visibilities which subtract too little window power
    filter_max_ps_win_ratio_sub = null
    // filter subtracted visibilities which subtract too much window power
    filter_min_ps_win_ratio_sub = null
    // filter visibilities with too much ionospheric activity
    filter_max_hyp_ionoqa = null

    // /////// //
    // imaging //
    // /////// //
    // -> dirty
    noimg = false
    noimgunsub = false
    img_suffix = ''
    img_weight = 'briggs -1.0'
    img_size = 4096
    img_scale = '40asec'
    img_channels_out = '4 -join-channels'
    img_intervals_out = '1'
    img_split_intervals = false
    img_split_coarse_chans = false
    img_pol = 'xx,yy,v'
    // -join-polarizations needs 2-4 polarizations
    wsclean_args = ''
    chgcentre_args = ''

    // -> dconv only
    nodeconv = false
    wsclean_dconv_args = '-multiscale'
    // https://wsclean.readthedocs.io/en/latest/basic_cleaning.html#threshold-and-maximum-number-of-iterations
    img_niter = 1000
    img_minor_clean_gain = 0.1
    // https://wsclean.readthedocs.io/en/latest/basic_cleaning.html#using-cotton-schwab-the-mgain-parameter
    img_major_clean_gain = 0.5
    // https://wsclean.readthedocs.io/en/latest/basic_cleaning.html#automatic-threshold
    img_auto_threshold = 1
    // https://wsclean.readthedocs.io/en/latest/masking.html
    img_auto_mask = 5
    // path containing `mwa_full_embedded_element_pattern.h5`, can be symlink
    img_mwa_path = '...'

    // -> previews
    krvis = false
    nothumbnail = false
    thumbnail_quantile = 0.95
    thumbnail_vmin = null
    thumbnail_vmax = null
    thumbnail_args = ''
    thumbnail_uvs = false
    thumbnail_psfs = false
    thumbnail_all_chans = false
    thumbnail_limits = true

    nopolcomp = true
    nomontage = true
    montage_by_pol = false
    montage_by_sub = false

    // -> videos:
    // by default, only make videos for MFS thumbnails, polcomps and polmontages.
    // -> make a video for each obsid, pol, img product scanning through channels
    frame_chan_scan = false
    // -> make a video for each channel, pol, img product scanning through obsids
    frame_obs_scan = false

    // -> imgqa
    noimgqa = false
    noplotimgqa = false

    // filter images with too much stokes V RMS
    filter_max_vrms_box_nosub = null
    // filter images with too little unsubtracted stokes V RMS
    filter_min_vrms_box_nosub = null
    // filter images with too much integrated unsubtracted pks V power compared to (XX+YY)
    filter_max_pks_int_v_ratio_nosub = null
    // filter images with too little integrated unsubtracted pks V power compared to (XX+YY)
    filter_min_pks_int_v_ratio_nosub = null
    // filter images with too much difference between xx and yy integrated pks flux
    filter_max_pks_int_diff_sub = null
    // filter images with too little difference between xx and yy integrated pks flux
    filter_min_pks_int_diff_sub = null
    // filter images which don't subtract enough integrated pks flux xx
    filter_max_pks_int_sub_ratio_xx = null
    // filter images which subtract too much integrated pks flux xx
    filter_min_pks_int_sub_ratio_xx = null
    // filter images which don't subtract enough integrated pks flux yy
    filter_max_pks_int_sub_ratio_yy = null
    // filter images which subtract too much integrated pks flux yy
    filter_min_pks_int_sub_ratio_yy = null

    noimgcombine = true
    nostackthumbs = true

    // //// //
    // peel //
    // //// //

    peel_suffix = ''

    // ///// //
    // chips //
    // ///// //
    nochips = false
    nochipscombine = true
    groupByPointing = false
    groupByLst = false
    // lssa individual obs without combine
    lssaObs = false
    // number of obs in each chunk ()
    chunkSize = 80
    // allow incomplete chunks
    chunkRemainder = true
    // limit number of chunks in each group
    chunkCount = 80
    // bias modes (0/10/11/12/13) - 0 = full, 10 = high
    // TODO: lssa_bias_modes = [0,10,12,13]
    lssa_bias_mode = 10
    // maximum u coordinate
    lssa_maxu = 300
    // number of bins
    lssa_nbins = 80
    // lssa binary name, e.g. 'lssa_fg_simple', 'lssa_fg_nfft', 'lssa_fg_general'
    lssa_bin = 'lssa_fg_general'

    // -> chips plotting
    noplotchips = false
    // -> k cuts
    kperp_max = 0.03
    kperp_min = 0.02
    kparra_min = 0.11
    kparra_max = null // default
    // -> optionally override cosmological constants
    omega_matter = null
    omega_baryon = null
    omega_lambda = null
    hubble = null
    density_correction = null
    // cmt
    k_edges = [0.00000, 0.0353895, 0.0707791, 0.106169, 0.141558, 0.176948, 0.212337, 0.247727, 0.283116, 0.318506, 0.353895, 0.389285, 0.424674, 0.460064, 0.495453, 0.530843, 0.566232, 0.601622, 0.637012, 0.672401, 0.707791, 0.743180, 0.778570, 0.813959, 0.849349, 0.884738, 0.920128, 0.955517, 0.990907, 1.02630, 1.06169, 1.09708, 1.13246, 1.16785, 1.20324, 1.23863, 1.27402, 1.30941, 1.34480, 1.38019, 1.41558, 1.45097, 1.48636, 1.52175, 1.55714, 1.59253, 1.62792, 1.66331, 1.69870, 1.73409]
    // cmt / 2
    // k_edges = [ 0.00000, 0.0707791, 0.141558, 0.212337, 0.283116, 0.353895, 0.424674, 0.495453, 0.566232, 0.637012, 0.707791, 0.778570, 0.849349, 0.920128, 0.990907, 1.06169, 1.13246, 1.20324, 1.27402, 1.34480, 1.41558, 1.48636, 1.55714, 1.62792, 1.69870 ]

    demo_prelude = null

    // results
    result_suffix = ''

    // novideo = false // use `--novideo=true` to skip video generation
    novideo = true

    // /////// //
    // archive //
    // /////// //
    // use `--tarchive=true` to create tarballs of qa files
    tarchive = false
    // use `--archive=true` to enable archiving
    archive = false
    // use `--archive_prep=true` to enable archiving of preprocessed vis
    archive_prep = false
    // use `--archive_uvfits=true` to enable archiving of calibrated vis
    archive_uvfits = false
    // prefix for acacia bucket names
    bucket_prefix = "mwaeor:high0"

    // //// //
    // lmod //
    // //// //
    singularity_lmod = 'singularity'
    rclone_lmod = 'rclone'
    squid_lmod = 'giant-squid/default'
    birli_lmod = "birli/default"
    hyperdrive_lmod = "hyperdrive/default"
    hyperdrive_cpu_lmod = "hyperdrive/default"
    wsclean_lmod = "wsclean/default"

    // ///// //
    // shims //
    // ///// //
    hyperdrive = 'hyperdrive'
    hyperdrive_cpu = 'hyperdrive'
    birli = 'birli'
    giant_squid = 'giant-squid'
    rclone = 'rclone'
    jq = 'jq'
    ps_metrics = "ps_metrics"
    proxy_prelude = ''
    casa = 'casa'
    wsclean = 'wsclean'
    chgcentre = 'chgcentre'
    mwa_reduce = ''

    // misc defaults
    num_gpus = 1

    // /////////// //
    // singularity //
    // /////////// //
    birli_sif = 'docker://mwatelescope/birli:latest'
    giant_squid_sif = 'docker://mwatelescope/giant-squid:latest'
    // hyperdrive_sif = 'docker://d3vnull0/mwa_hyperdrive:peel'
    // hyperdrive_sif = 'docker://d3vnull0/mwa_hyperdrive:dev-cuda11.4.3-cascadelake'
    // hyperdrive_sif = 'docker://d3vnull0/mwa_hyperdrive:peel-cuda11.4.3-cc70-x86-64-v2'
    hyperdrive_sif = 'docker://d3vnull0/mwa_hyperdrive:peel-experiment'
    mwa_qa_sif = 'docker://d3vnull0/mwa_qa:latest'
    fitcal_sif = 'docker://d3vnull0/fitcal:latest'
    tap_sif = 'docker://d3vnull0/tap:latest'
    cotter_sif = 'docker://mwatelescope/cotter:latest'
    casa_sif = 'docker://d3vnull0/casa:latest'
    wsclean_sif = 'docker://paulhancock/wsclean:2.10.0-build-1'
    ffmpeg_sif = 'docker://jrottenberg/ffmpeg:latest'
    mwa_reduce_sif = ''
    imagemagick_sif = 'docker://dpokidov/imagemagick:latest'
    ssins_sif = 'docker://d3vnull0/ssins:latest'
    mwax_mover_sif = 'docker://d3vnull0/mwax_mover:latest'
    // chips_sif = 'docker://d3vnull0/chips:latest'
    chips_sif = 'docker://d3vnull0/chips2024:latest'
    chips_wrappers_sif = 'docker://d3vnull0/chips_wrappers:latest'
    mwa_demo_sif = 'docker://mwatelescope/mwa-demo:latest-amd64'

    // give jobs longer runtimes to account for filesystem mismanagement
    scratchFactor = 1

    // command for counting the nubmer of GPUs
    cmd_gpu_count = "nvidia-smi -L | wc -l"
}
process {
    errorStrategy = 'ignore'
    afterScript = 'set -x; ls -alh'
    // cache = false
    // global per-process rate limit
    maxForks = 60
    withLabel: rate_limit {
        maxForks = 5
    }
    withLabel: rate_limit_20 {
        maxForks = 20
    }
    withLabel: rate_limit_50 {
        maxForks = 50
    }
    withLabel: mwa_demo {
        container = "${params.mwa_demo_sif}"
    }
}
executor {
    exitReadTimeout = '5min'
    queueSize = 10
    jobName = { "nf-${task.process.split(':')[-1]}.${task.tag}" }
}
profiles {
    setonix {
        // docs https://mwatelescope.atlassian.net/wiki/spaces/MP/pages/472612865/Tutorial+Visibility+Data+Reduction+on+Setonix
        // HPE Cray EX 235A
        // work (1592):
        // - 2 * EPYC 7763, “Milan” 64-Core 2.45GHz,
        // - 256 GB RAM
        // highmem (8) has 1TB RAM
        // gpu (154):
        // - 1 * EPYC 7A53 “Trento” 64-Core
        // - 8 * AMD Instinct MI250X GPUs
        // - 256 GB RAM
        // gpu-highmem (38) has 512GB RAM
        // no nvme!
        // - work: 1840MB per cpu
        // - highmem: 7900MB per cpu
        // - gpu: 58880MB per gpu

        // cd $MYSOFTWARE/MWAEoR-Pipeline; module load java/default singularity/default spack/default; nextflow run main.nf -profile setonix -w $MYSCRATCH/nfwork -c eor0high-dn-2024-04-03.conf --nothumbnail --nopolcomp --thumbnail_limits=false --novideo --obsids_suffix=-test --outdir=/scratch/${PAWSEY_PROJECT}/${USER}/nftest/
        singularity {
            cacheDir = "/scratch/${PAWSEY_PROJECT}/${USER}/singularity"
            runOptions = '--home ${PWD} --cleanenv --bind /scratch --bind /software'
            enabled = true
            autoMounts = true
        }
        spack {
            enabled = true
        }
        executor {
            queueSize = 100
            submitRateLimit = '20/1min'
            jobName = { "nf-${task.process.split(':')[-1]}.${task.tag}" }
        }
        workDir = "${MYSCRATCH}/nfwork"
        params {
            outdir = "${MYSCRATCH}/nfdata"
            resultsdir = "${MYSCRATCH}/nfresults"

            // program shims
            hyperdrive_sif = 'docker://d3vnull0/hyperdrive:v0.3.0-setonix-rocm6.1'
            singularity_lmod = 'singularity/default'
            spack_lmod = 'spack/default'
            rclone_lmod = 'rclone/default'
            squid_lmod = "giant-squid/1.1.0"
            // birli_lmod = "birli/van_vleck-iqa7dmx"
            birli_lmod = "birli/0.15.1"
            hyperdrive_lmod = "hyperdrive-amd-gfx90a/0.4.1"
            hyperdrive_cpu_lmod = "hyperdrive/0.4.1"
            wsclean_lmod = "wsclean/3.4-idg-everybeam"

            singularity_prefix = '[ $(command -v singularity) ] || ' + "module load ${params.singularity_lmod}; singularity exec --bind /scratch --bind /software"
            giant_squid = "giant-squid"
            rclone = "rclone"
            jq = "/software/projects/mwaeor/dev/bin/jq"
            casa = '' + "${params.singularity_prefix}" + ' --bind ${PWD} --writable-tmpfs --pwd ${PWD} --home ${PWD} --cleanenv ' + "${params.casa_sif} casa"

            // calibration
            sourcelist = '/software/projects/mwaeor/dev/srclist_pumav3_EoR0LoBES_EoR1pietro_CenA-GP_2023-11-07.yaml'
            // https://github.com/JLBLine/srclists/blob/master/srclist_pumav3_EoR0LoBES_EoR1pietro_CenA-GP_2023-11-07.fits
            beam_path = '/software/projects/mwaeor/dev/MWA_embedded_element_pattern_rev2_interp_167_197MHz.h5'
            img_mwa_path = '/software/projects/mwaeor/dev/'

            scratchFactor = 2

            // cmd_gpu_count = "rocm-smi -a --json | jq -r '.|keys|.[]|select(startswith(\"card\"))' | wc -l"
            cmd_gpu_count = "echo 1"
        }
        process {
            executor = 'slurm'
            queue = 'work'
            // about 1/128 of a node
            cpus = 2
            // about 1/18 of a node
            memory = 16.GB
            time = 15.minute
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}" }
            scratch = 'ram-disk'
            stageInMode = 'symlink'
            stageOutMode = 'move'
            beforeScript = 'set -x; chown ${USER}:${PAWSEY_PROJECT} ${NXF_SCRATCH:="."}; chmod g+rs ${NXF_SCRATCH:="."}; hostname; env | grep SLURM; sleep $((2 + $RANDOM % 15)); module load spack/default; date -Is; scontrol -d show node $SLURM_JOB_NODELIST'

            withLabel: cpu_quarter {
                cpus = 16
            }
            withLabel: cpu_half {
                cpus = 32
            }
            withLabel: cpu_full {
                cpus = 64
            }
            withLabel: mem_tiny {
                memory = 1024.MB
            }
            withLabel: mem_quarter {
                memory = 58880.MB
            }
            withLabel: mem_half {
                memory = 117760.MB
            }
            withLabel: mem_full {
                memory = 235520.MB
            }
            // only 2 jobs at a time with highmem
            withLabel: mem_super {
                queue = 'highmem'
                memory = 505260.MB
                maxForks = 2
            }
            withLabel: mem_ultra {
                queue = 'highmem'
                memory = 1010520.MB
                maxForks = 2
            }
            // no nvme!
            withLabel: gpu {
                queue = 'gpu'
                cpus = null
                memory = null
                clusterOptions = { "--nodes=1 --account=${PAWSEY_PROJECT}-gpu --gres=gpu:1" }
            }
            withLabel: gpu_nvme {
                queue = 'gpu-highmem'
                cpus = null
                memory = null
                clusterOptions = { "--nodes=1 --account=${PAWSEY_PROJECT}-gpu --gres=gpu:8" }
            }
            // withLabel: 'birli|hyperdrive|wsclean' {
            //     module = "spack/default"
            //     spack = "/software/projects/mwaeor/dev/setonix/2024.05/environments/astrospecs-dev/spack.yaml"
            //     container = null
            // }
            withLabel: birli {
                module = "${params.birli_lmod}"
                container = null
            }
            withLabel: hyperdrive {
                module = { "${params.hyperdrive_lmod}" + (params.pullCalSol ? ":${params.rclone_lmod}" : "") }
                container = null
            }
            withLabel: hyperdrive_cpu {
                module = { "${params.hyperdrive_cpu_lmod}" }
                container = null
            }
            withLabel: wsclean {
                module = "${params.wsclean_lmod}"
                container = null
            }
            withLabel: mwa_reduce {
                module = "${params.singularity_lmod}"
                container = "${params.mwa_reduce_sif}"
            }
            // withLabel: python {
            //     module = "${params.singularity_lmod}"
            //     container = "${params.mwa_qa_sif}"
            // }
            withLabel: fitcal {
                module = "${params.singularity_lmod}"
                container = "${params.fitcal_sif}"
            }
            withLabel: casa {
                module = "${params.singularity_lmod}"
                container = "${params.casa_sif}"
            }
            // withLabel: tap {
            //     module = "${params.singularity_lmod}"
            //     container = "${params.tap_sif}"
            // }
            withLabel: giant_squid {
                module = { "${params.squid_lmod}" + (params.pullPrep ? ":${params.rclone_lmod}" : "") }
            }
            withLabel: chips {
                module = "${params.singularity_lmod}"
                container = "${params.chips_sif}"
                afterScript = 'for log in $(ls -t syslog*.txt *.log); do echo "-> head $log"; head -n 100 $log; echo "-> tail $log"; tail -n 100 $log; done; ls -alh; du -hd0'
            }
            withLabel: chips_wrappers {
                module = "${params.singularity_lmod}"
                container = "${params.chips_wrappers_sif}"
            }
            withLabel: ffmpeg {
                module = "${params.singularity_lmod}"
                container = "${params.ffmpeg_sif}"
            }
            withLabel: imagemagick {
                module = "${params.singularity_lmod}"
                container = "${params.imagemagick_sif}"
            }
            // withLabel: ssins {
            //     module = "${params.singularity_lmod}"
            //     container = "${params.ssins_sif}"
            // }
            withLabel: mwax_mover {
                module = "${params.singularity_lmod}"
                container = "${params.mwax_mover_sif}"
            }
            withLabel: 'mwa_demo|python|tap|ssins' {
                module = "${params.singularity_lmod}"
                container = "${params.mwa_demo_sif}"
            }
        }
    }
    garrawarla {
        // workq and gpuq:
        // - 78 * HPE ProLiant XL190r Gen10 Servers
        // - 2 * Xeon Gold 6230 “Cascade Lake” (20 core, 2.1GHz CPU)
        // - 1 * NVIDIA Tesla V100 Tensor Core 32GB GPU
        // - 38 usable cores when gpu enabled
        // - 384 GB of DDR4-2933 RAM (360G usable)
        // - 960GB NVMe drive (890G usable)
        singularity {
            cacheDir = "/scratch/${PAWSEY_PROJECT}/${USER}/singularity"
            runOptions = '--home ${PWD} --cleanenv --bind /scratch'
            enabled = true
            autoMounts = true
        }
        executor {
            queueSize = 100
            submitRateLimit = '20/1min'
            jobName = { "nf-${task.process.split(':')[-1]}.${task.tag}" }
        }
        workDir = "${MYSCRATCH}/nfwork"
        params {
            outdir = "${MYSCRATCH}/nfdata"
            resultsdir = "${MYSCRATCH}/nfresults"

            hyperdrive_lmod = "hyperdrive/peel-experiment"
            hyperdrive_cpu_lmod = "hyperdrive/peel-experiment"

            // binaries
            singularity_prefix = '[ $(command -v singularity) ] || module load singularity; singularity exec --bind /scratch --bind /pawsey'
            // hyperdrive = 'module use /pawsey/mwa/software/python3/modulefiles; module load hyperdrive; hyperdrive'
            birli = "${params.singularity_prefix}" + ' --bind ${PWD}:/tmp --writable-tmpfs --pwd /tmp --home ${PWD} ' + " --cleanenv ${params.birli_sif} /opt/cargo/bin/birli"
            // giant_squid = "${params.singularity_prefix} ${params.giant_squid_sif} /opt/cargo/bin/giant-squid"
            giant_squid = "/pawsey/mwa/bin/giant-squid"
            rclone = "/pawsey/mwa/bin/rclone"
            jq = "/pawsey/mwa/bin/jq"
            // wsclean = '${params.singularity_prefix} ${params.wsclean_sif} wsclean'
            casa = "${params.singularity_prefix}" + ' --bind ${PWD}:/tmp --writable-tmpfs --pwd /tmp --home ${PWD} ' + " --cleanenv ${params.casa_sif} casa"
            ps_metrics = 'ps_metrics'
            mwa_reduce = "singularity exec -B /pawsey/mwa:/usr/lib/python3/dist-packages/mwapy/data ${params.mwa_reduce_sif}"

            // calibration
            // sourcelist = '/pawsey/mwa/software/python3/srclists/master/srclist_pumav3_EoR0LoBESv2_fixedEoR1pietro+ForA_phase1+2_edit.txt'
            // sourcelist = '/pawsey/mwa/mwaeor/dev/srclists/srclist_pumav3_EoR0LoBESv2_fixedEoR1pietro+ForA_phase1+2_edit.txt'
            sourcelist = '/pawsey/mwa/mwaeor/dev/LoBES/srclist_pumav3_EoR0LoBES_EoR1pietro_CenA-GP_2023-11-07.yaml'
            // https://github.com/JLBLine/srclists/blob/master/srclist_pumav3_EoR0LoBES_EoR1pietro_CenA-GP_2023-11-07.fits
            // beam_path = '/pawsey/mwa/mwa_full_embedded_element_pattern.h5'
            beam_path = '/pawsey/mwa/MWA_embedded_element_pattern_rev2_interp_167_197MHz.h5'
            img_mwa_path = '/pawsey/mwa'

            singularity_lmod = 'singularity'
            scratchFactor = 4
        }
        process {
            executor = 'slurm'
            queue = 'workq'
            cpus = 2
            // about 1/20 of a node
            memory = 16.GB
            // about 1/20 of a node
            time = { 15.minute * params.scratchFactor }
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}" }
            scratch = 'ram-disk'
            stageInMode = 'copy'
            stageOutMode = 'move'
            beforeScript = 'set -x; chown ${USER}:${PAWSEY_PROJECT} ${NXF_SCRATCH:="."}; chmod g+rs ${NXF_SCRATCH:="."}; module use /pawsey/mwa/software/python3/modulefiles; module use /astro/mwaeor/software/modulefiles; hostname; env | grep SLURM; sleep $((2 + $RANDOM % 15)); date -Is'
            // jobs which actually make use of multiprocessing
            withLabel: cpu_quarter {
                cpus = 9
            }
            withLabel: cpu_half {
                cpus = 18
            }
            // jobs which will need maximum cpu count
            withLabel: cpu_full {
                cpus = 36
            }
            withLabel: mem_tiny {
                memory = 1.GB
            }
            // jobs which need a bit more memory
            withLabel: mem_quarter {
                memory = 90.GB
            }
            withLabel: mem_half {
                memory = 180.GB
            }
            // jobs which will need maximum memory
            withLabel: 'mem_full|mem_super|mem_ultra' {
                memory = 360.GB
            }
            // processes which handle big files that don't fit in ramdisk
            withLabel: nvme {
                scratch = "/nvmetmp/"
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT} --tmp=100G" }
            }
            withLabel: nvme_full {
                scratch = "/nvmetmp/"
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT} --tmp=880G" }
            }
            withLabel: gpu {
                queue = 'gpuq'
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}  --gres=gpu:1" }
            }
            withLabel: gpu_nvme {
                scratch = "/nvmetmp/"
                queue = 'gpuq'
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}  --gres=gpu:1 --tmp=100G" }
            }
            // salloc --nodes=1 --mem=1G --time=1:00:00 --clusters=setonix --partition=copy --account=mwaeor --tasks 1 --cpus-per-task=1
            withLabel: datamover {
                scratch = 'ram-disk'
                stageInMode = "symlink"
                module = 'rclone'
                cpus = 1
                maxForks = 8
                memory = 8.GB
                time = { 2.hour * params.scratchFactor }
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}" }
                queue = 'copyq'
            }
            withLabel: datamover_dl {
                scratch = 'ram-disk'
                stageInMode = "symlink"
                module = 'rclone'
                cpus = 1
                maxForks = 8
                memory = 31500.MB
                time = { 2.hour * params.scratchFactor }
                clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}" }
                queue = 'copyq'
            }
            withLabel: 'hyperdrive|hyperdrive_cpu' {
                module = "${params.hyperdrive_lmod}"
            }
            withLabel: wsclean {
                module = 'wsclean/2.9'
            }
            withLabel: mwa_reduce {
                module = "${params.singularity_lmod}"
                container = "${params.mwa_reduce_sif}"
            }
            withLabel: python {
                module = "${params.singularity_lmod}"
                container = "${params.mwa_qa_sif}"
            }
            withLabel: fitcal {
                module = "${params.singularity_lmod}"
                container = "${params.fitcal_sif}"
            }
            withLabel: casa {
                module = "${params.singularity_lmod}"
                container = "${params.casa_sif}"
            }
            withLabel: tap {
                module = "${params.singularity_lmod}"
                container = "${params.tap_sif}"
            }
            withLabel: rclone {
                module = 'rclone'
            }
            withLabel: chips {
                module = "${params.singularity_lmod}"
                container = "${params.chips_sif}"
                afterScript = 'for log in $(ls -t syslog*.txt *.log); do echo "-> head $log"; head -n 100 $log; echo "-> tail $log"; tail -n 100 $log; done; ls -alh; du -hd0'
            }
            withLabel: chips_wrappers {
                module = "${params.singularity_lmod}"
                container = "${params.chips_wrappers_sif}"
            }
            withLabel: ffmpeg {
                module = "${params.singularity_lmod}"
                container = "${params.ffmpeg_sif}"
            }
            withLabel: imagemagick {
                module = "${params.singularity_lmod}"
                container = "${params.imagemagick_sif}"
            }
            withLabel: ssins {
                module = "${params.singularity_lmod}"
                container = "${params.ssins_sif}"
            }
            withLabel: mwax_mover {
                module = "${params.singularity_lmod}"
                container = "${params.mwax_mover_sif}"
            }
            withLabel: mwa_demo {
                module = "${params.singularity_lmod}"
                container = "${params.mwa_demo_sif}"
            }
        }
    }
    dug {
        singularity {
            cacheDir = '/data/curtin_mwaeor/singularity'
        }
        executor {
            queueSize = 200
            // submitRateLimit = '1/30s'
            jobName = { "nf-${task.process.split(':')[-1]}.${task.tag}" }
            exitReadTimeout = '5min'
        }
        process {
            scratch = '$TMPDIR_SHM'
            executor = 'slurm'
            queue = 'curtin_mwaeor'
            cpus = 18
            memory = '60G'
            clusterOptions = '--constraint=knl&nogpu'
            stageInMode = 'copy'
            stageOutMode = 'copy'
            withLabel: cpu {
                cpus = 36
            }
            withLabel: wsclean {
                clusterOptions = '--constraint=clx&nogpu'
            }
            withLabel: hyperdrive {
                module = 'cuda/11.3.1:gcc-rt/9.2.0'
                clusterOptions = "--constraint='v100'"
            }
            withLabel: python {
                module = 'miniconda/4.8.3'
                conda = '/data/curtin_mwaeor/sw/conda/dev'
            }
            withLabel: rclone {
                module = 'rclone'
            }
            withLabel: chips {
                module = 'cfitsio/3.470'
            }
            withLabel: python {
                conda = "${params.astro_conda}"
            }
        }
        params {
            outdir = '/data/curtin_mwaeor/data'
            proxy_prelude = 'export http_proxy="http://proxy.per.dug.com:3128" https_proxy="http://proxy.per.dug.com:3128" all_proxy="proxy.per.dug.com:3128" ftp_proxy="http://proxy.per.dug.com:3128"'

            // sifs
            // birli_sif = '/data/curtin_mwaeor/sw/singularity/birli/birli_mwaf.sif'
            // mwa_qa_sif = '/data/curtin_mwaeor/sw/singularity/mwa_qa/mwa_qa_latest.sif'
            // cotter_sif = '/data/curtin_mwaeor/sw/singularity/cotter/cotter_latest.sif'
            // casa_sif = '/data/curtin_mwaeor/sw/singularity/casa/casa_latest.sif'

            // binaries
            hyperdrive = '/data/curtin_mwaeor/sw/bin/hyperdrive'
            birli = 'singularity exec --bind ${PWD}:/tmp --writable-tmpfs --pwd /tmp --home ${PWD} --cleanenv ' + " ${params.birli_sif} /opt/cargo/bin/birli"
            giant_squid = '/data/curtin_mwaeor/sw/bin/giant-squid'
            jq = '/data/curtin_mwaeor/sw/bin/jq'
            wsclean = '/data/curtin_mwaeor/sw/bin/wsclean'
            // wsclean = '/data/curtin_mwaeor/sw/wsclean/3.1-knl-icc/bin/wsclean'
            // wsclean = 'module use /data/curtin_mwaeor/sw/modules; module load wsclean/3.1-knl-icc; /data/curtin_mwaeor/sw/wsclean/3.1-knl-icc/bin/wsclean -j 60'
            // wsclean = 'module use /data/curtin_mwaeor/sw/modules; module load wsclean/3.1-knl-icc; mpirun -np 30 /data/curtin_mwaeor/sw/wsclean/3.1-knl-icc/bin/wsclean-mp'
            casa = 'singularity exec --bind ${PWD}:/tmp --writable-tmpfs --pwd /tmp --home ${PWD} --cleanenv ' + " ${params.casa_sif} /usr/bin/casa"
            ps_metrics = "/data/curtin_mwaeor/src/chips/src/ps_metrics"

            // calibration
            sourcelist = '/data/curtin_mwaeor/data/srclist_pumav3_EoR0LoBESv2_fixedEoR1pietro+ForA_phase1+2_edit.txt'
            beam_path = '/data/curtin_mwaeor/data/mwa_full_embedded_element_pattern.h5'
            // todo: can't determine this from nvidia-smi
            // v100 => 70, a100 => 80
            cuda_compute = 70
        }
    }
    genericHPC {
        singularity {
            cacheDir = '.'
            enabled = true
            autoMounts = true
        }
        params {
            outdir = './nfdata'
            resultsdir = './nfresults'
        }
        process {
            executor = 'slurm'
            queue = 'workq'
            cpus = 2
            // about 1/18 of a node
            memory = 10.GB
            // about 1/18 of a node
            time = 15.minute
            clusterOptions = { "--nodes=1 --cpus-per-task=${task.cpus} --account=${PAWSEY_PROJECT}" }
            scratch = 'ram-disk'
            stageInMode = 'copy'
            stageOutMode = 'move'
            beforeScript = 'set -x; chown ${USER}:${PAWSEY_PROJECT} ${NXF_SCRATCH:="."}; chmod g+rs ${NXF_SCRATCH:="."}; module use /pawsey/mwa/software/python3/modulefiles; module use /astro/mwaeor/software/modulefiles; hostname; env | grep SLURM; sleep $((2 + $RANDOM % 15)); date -Is'
            errorStrategy = 'terminate'
            withLabel: python {
                module = 'singularity'
                container = "${params.mwa_qa_sif}"
            }
        }
    }
    sirius {
        process {
            scratch = '/tmp/mwaeor'
            executor = 'local'
        }
        params {
            outdir = '/data/dev'
            hyperdrive = '/usr/local/bin/hyperdrive'
            sourcelist = '/data/dev/calibration/srclist_pumav3_EoR0LoBESv2_fixedEoR1pietro+ForA_phase1+2.txt'
            beam_path = '/data/dev/calibration/mwa_full_embedded_element_pattern.h5'
            cuda_compute = 75
        }
    }
    gacrux {
        singularity {
            cacheDir = "/data/${USER}/.singularity"
            enabled = true
            runOptions = '--bind /data --bind /cygnus'
        }
        process {
            scratch = '/tmp/mwaeor'
            executor = 'local'
            beforeScript = 'set -x; pwd; date -Is'
            maxForks = 1

            withLabel: rate_limit {
                maxForks = 1
            }
            withLabel: rate_limit_20 {
                maxForks = 2
            }
            withLabel: rate_limit_50 {
                maxForks = 5
            }
            withLabel: 'mem_quarter|cpu_quarter' {
                maxForks = 4
            }
            withLabel: 'mem_half|cpu_half' {
                maxForks = 2
            }
            withLabel: 'mem_full|cpu_full' {
                maxForks = 1
            }
            withLabel: mwa_reduce {
                container = "${params.mwa_reduce_sif}"
            }
            withLabel: 'mwa_demo|ssins' {
                maxForks = 5
                container = null
            }
            withLabel: fitcal {
                container = "${params.fitcal_sif}"
            }
            withLabel: chips {
                container = "${params.chips_sif}"
            }
            withLabel: chips_wrappers {
                container = "${params.chips_wrappers_sif}"
            }
            withLabel: ffmpeg {
                container = "${params.ffmpeg_sif}"
            }
            withLabel: imagemagick {
                container = "${params.imagemagick_sif}"
            }
            withLabel: mwax_mover {
                container = "${params.mwax_mover_sif}"
            }
        }
        executor {
            queueSize = 10
        }
        params {
            // outdir = '/data/dev'
            outdir = '/cygnus/dev'
            resultsdir = '/data/dev/nfresults'
            demo_prelude = '. /home/dev/src/mwa-nextflow-starter/.venv/bin/activate'
            // wsclean = '/usr/bin/wsclean'
            // nouv = true
            // sourcelist = '/opt/cal/srclist_pumav3_EoR0LoBESv2_fixedEoR1pietro+ForA_phase1+2_edit.txt'
            sourcelist = '/opt/cal/srclist_pumav3_EoR0LoBES_EoR1pietro_CenA-GP_2023-11-07.yaml'
            beam_path = '/opt/cal/mwa_full_embedded_element_pattern.h5'
            img_mwa_path = '/opt/cal/'
            cuda_compute = 86
        }
    }
}
report {
    overwrite = true
}
